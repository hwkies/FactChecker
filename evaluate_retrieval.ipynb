{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search import get_topics, get_qrels, get_qrels_file\n",
    "from index import RetrievalModel\n",
    "import json\n",
    "import time\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_topics = get_topics('beir-v1.0.0-climate-fever-test')\n",
    "covid_topics = get_topics('beir-v1.0.0-trec-covid-test')\n",
    "fever_topics = get_topics('beir-v1.0.0-fever-test')\n",
    "news_topics = get_topics('beir-v1.0.0-trec-news-test')\n",
    "scifact_topics = get_topics('beir-v1.0.0-scifact-test')\n",
    "\n",
    "climate_topics = {k: climate_topics[k] for k in list(climate_topics)[:5]}\n",
    "covid_topics = {k: covid_topics[k] for k in list(covid_topics)[:5]}\n",
    "fever_topics = {k: fever_topics[k] for k in list(fever_topics)[:5]}\n",
    "news_topics = {k: news_topics[k] for k in list(news_topics)[:5]}\n",
    "scifact_topics = {k: scifact_topics[k] for k in list(scifact_topics)[:5]}\n",
    "\n",
    "def safe_get_qrels(dataset):\n",
    "    try:\n",
    "        return get_qrels(dataset)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting qrels for {dataset}: {e}\")\n",
    "        # Below code pulled from pyserini/pyserini/search/__base.py\n",
    "        # Needed to rewrite to open file with utf-8 because of unrecognized characters\n",
    "        qrels_file_path = get_qrels_file(dataset)\n",
    "        qrels = {}\n",
    "        with open(qrels_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                qid, _, docid, judgement = line.rstrip().split()\n",
    "                \n",
    "                if qid.isdigit():\n",
    "                    qrels_key = int(qid)\n",
    "                else:\n",
    "                    qrels_key = qid\n",
    "                    \n",
    "                if docid.isdigit():\n",
    "                    doc_key = int(docid)\n",
    "                else:\n",
    "                    doc_key = docid\n",
    "                    \n",
    "                if qrels_key in qrels:\n",
    "                    qrels[qrels_key][doc_key] = judgement\n",
    "                else:\n",
    "                    qrels[qrels_key] = {doc_key: judgement}\n",
    "        return qrels\n",
    "\n",
    "climate_qrels = safe_get_qrels('beir-v1.0.0-climate-fever-test')\n",
    "covid_qrels = safe_get_qrels('beir-v1.0.0-trec-covid-test')\n",
    "fever_qrels = safe_get_qrels('beir-v1.0.0-fever-test')\n",
    "news_qrels = safe_get_qrels('beir-v1.0.0-trec-news-test')\n",
    "scifact_qrels = safe_get_qrels('beir-v1.0.0-scifact-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_precision_sum(topics: dict, qrels: dict, k=100):\n",
    "    averagePrecisionSum = 0\n",
    "    for queryid, query in topics.items():\n",
    "        hits = RetrievalModel(query['title']).retrieve_without_standings()[:k]\n",
    "        precisionSum = 0\n",
    "        relevantCount = 0\n",
    "        for i, hit in enumerate(hits):\n",
    "            if hit['docid'] in qrels[queryid]:\n",
    "                relevantCount += 1\n",
    "                precisionSum += relevantCount / (i + 1)\n",
    "        if relevantCount > 0:\n",
    "            averagePrecisionSum += precisionSum / relevantCount\n",
    "        else:\n",
    "            averagePrecisionSum += 0\n",
    "    return averagePrecisionSum\n",
    "\n",
    "def get_recall_at_k(topics: dict, qrels: dict, k=100):\n",
    "    total_relevant_docs = 0\n",
    "    retrieved_relevant_docs = 0\n",
    "    for queryid, query in topics.items():\n",
    "        total_query_relevant = sum(1 for rel in qrels[queryid].values() if int(rel) > 0)\n",
    "        total_relevant_docs += total_query_relevant\n",
    "        hits = RetrievalModel(query['title']).retrieve_without_standings()[:k]\n",
    "        query_retrieved_relevant = sum(1 for hit in hits if hit['docid'] in qrels[queryid] and int(qrels[queryid][hit['docid']]) > 0)\n",
    "        retrieved_relevant_docs += query_retrieved_relevant\n",
    "    recall_at_100 = retrieved_relevant_docs / total_relevant_docs if total_relevant_docs > 0 else 0\n",
    "    return recall_at_100\n",
    "\n",
    "def get_f1_score_at_k(topics: dict, qrels: dict, k=100):\n",
    "    total_f1_score = 0\n",
    "    num_queries = len(topics)\n",
    "    for queryid, query in topics.items():\n",
    "        hits = RetrievalModel(query['title']).retrieve_without_standings()[:k]\n",
    "        total_relevant = sum(1 for rel in qrels[queryid].values() if int(rel) > 0)\n",
    "        retrieved_relevant = sum(1 for hit in hits if hit['docid'] in qrels[queryid] and int(qrels[queryid][hit['docid']]) > 0)\n",
    "        precision = retrieved_relevant / k if k > 0 else 0\n",
    "        recall = retrieved_relevant / total_relevant if total_relevant > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "        total_f1_score += f1_score\n",
    "    average_f1_score = total_f1_score / num_queries\n",
    "\n",
    "    return average_f1_score\n",
    "\n",
    "def measure_retrieval_time(topics: dict, num_runs=5, k=100):\n",
    "    all_query_times = []\n",
    "\n",
    "    for queryid, query in topics.items():\n",
    "        query_times = []\n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.perf_counter()\n",
    "            hits = RetrievalModel(query['title']).retrieve()[:k]\n",
    "            end_time = time.perf_counter()\n",
    "            query_times.append((end_time - start_time) * 1000)\n",
    "        all_query_times.append(statistics.mean(query_times))\n",
    "    overall_mean_time = statistics.mean(all_query_times)\n",
    "    overall_median_time = statistics.median(all_query_times)\n",
    "    overall_min_time = min(all_query_times)\n",
    "    overall_max_time = max(all_query_times)\n",
    "    overall_std_dev = statistics.stdev(all_query_times) if len(all_query_times) > 1 else 0\n",
    "\n",
    "    return {\n",
    "        'mean_time': overall_mean_time,\n",
    "        'median_time': overall_median_time,\n",
    "        'min_time': overall_min_time,\n",
    "        'max_time': overall_max_time,\n",
    "        'std_dev': overall_std_dev\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averagePrecisionSum = 0\n",
    "averagePrecisionSum += get_average_precision_sum(climate_topics, climate_qrels) + get_average_precision_sum(covid_topics, covid_qrels) + get_average_precision_sum(fever_topics, fever_qrels) + get_average_precision_sum(news_topics, news_qrels) + get_average_precision_sum(scifact_topics, scifact_qrels)\n",
    "lenTopics = len(climate_topics) + len(covid_topics) + len(fever_topics) + len(news_topics) + len(scifact_topics)\n",
    "meanAveragePrecision = averagePrecisionSum / lenTopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averageRecall = get_recall_at_k(climate_topics, climate_qrels) + get_recall_at_k(covid_topics, covid_qrels) + get_recall_at_k(fever_topics, fever_qrels) + get_recall_at_k(news_topics, news_qrels) + get_recall_at_k(scifact_topics, scifact_qrels) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1Score = get_f1_score_at_k(climate_topics, climate_qrels) + get_f1_score_at_k(covid_topics, covid_qrels) + get_f1_score_at_k(fever_topics, fever_qrels) + get_f1_score_at_k(news_topics, news_qrels) + get_f1_score_at_k(scifact_topics, scifact_qrels) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeStats = measure_retrieval_time(climate_topics, num_runs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonResult = {\n",
    "    \"MAP@100\": meanAveragePrecision, \n",
    "    \"Recall@100\": averageRecall, \n",
    "    \"F1@100\": f1Score,\n",
    "    \"time_stats\": timeStats\n",
    "    }\n",
    "print(jsonResult)\n",
    "\n",
    "with open(f\"eval_retrieval_metrics.json\", \"w\") as f:\n",
    "    json.dump(jsonResult, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
